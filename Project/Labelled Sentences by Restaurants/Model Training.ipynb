{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                      \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"/Users/charlotte/Desktop/BT4222/train_labelled_final.csv\"\n",
    "# reviews = pd.read_csv(path)\n",
    "reviews = pd.read_csv('./train_labelled.csv', encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Happy Tummy</td>\n",
       "      <td>Fresh ingredients, friendly peeps and so much ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-04-06T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Cibo Italiano</td>\n",
       "      <td>A small selection of Italian wines by the glas...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-12-24T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Yan kee Noodle House</td>\n",
       "      <td>The plus point is that the price remains the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-12-28T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Clinton Street Baking Company &amp; Restaurant</td>\n",
       "      <td>Same for more?I ordered what I thought was the...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-03T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Song Fa Bak Kut Teh</td>\n",
       "      <td>I will probably get that again.Have been very ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-01-05T00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Restaurant  \\\n",
       "0                                 Happy Tummy   \n",
       "1                               Cibo Italiano   \n",
       "2                        Yan kee Noodle House   \n",
       "3  Clinton Street Baking Company & Restaurant   \n",
       "4                         Song Fa Bak Kut Teh   \n",
       "\n",
       "                                              Review  Label  Stars  \\\n",
       "0  Fresh ingredients, friendly peeps and so much ...      1      5   \n",
       "1  A small selection of Italian wines by the glas...      1      4   \n",
       "2  The plus point is that the price remains the s...      1      4   \n",
       "3  Same for more?I ordered what I thought was the...      1      2   \n",
       "4  I will probably get that again.Have been very ...      5      5   \n",
       "\n",
       "                  Date  \n",
       "0  2016-04-06T00:00:00  \n",
       "1  2015-12-24T00:00:00  \n",
       "2  2018-12-28T00:00:00  \n",
       "3  2018-03-03T00:00:00  \n",
       "4  2019-01-05T00:00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    916\n",
       "5    645\n",
       "3    525\n",
       "1    394\n",
       "4    350\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Happy Tummy</td>\n",
       "      <td>Fresh ingredients, friendly peeps and so much ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-04-06T00:00:00</td>\n",
       "      <td>fresh ingredients friendly peeps much cheaper ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Cibo Italiano</td>\n",
       "      <td>A small selection of Italian wines by the glas...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-12-24T00:00:00</td>\n",
       "      <td>small selection italian wines glass beers well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Yan kee Noodle House</td>\n",
       "      <td>The plus point is that the price remains the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-12-28T00:00:00</td>\n",
       "      <td>plus point price remains 4 per bowl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Clinton Street Baking Company &amp; Restaurant</td>\n",
       "      <td>Same for more?I ordered what I thought was the...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-03T00:00:00</td>\n",
       "      <td>ordered thought signature omelette name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Song Fa Bak Kut Teh</td>\n",
       "      <td>I will probably get that again.Have been very ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-01-05T00:00:00</td>\n",
       "      <td>probably get tempted pick soup base sell sure ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Restaurant  \\\n",
       "0                                 Happy Tummy   \n",
       "1                               Cibo Italiano   \n",
       "2                        Yan kee Noodle House   \n",
       "3  Clinton Street Baking Company & Restaurant   \n",
       "4                         Song Fa Bak Kut Teh   \n",
       "\n",
       "                                              Review  Label  Stars  \\\n",
       "0  Fresh ingredients, friendly peeps and so much ...      1      5   \n",
       "1  A small selection of Italian wines by the glas...      1      4   \n",
       "2  The plus point is that the price remains the s...      1      4   \n",
       "3  Same for more?I ordered what I thought was the...      1      2   \n",
       "4  I will probably get that again.Have been very ...      5      5   \n",
       "\n",
       "                  Date                                       Review_clean  \n",
       "0  2016-04-06T00:00:00  fresh ingredients friendly peeps much cheaper ...  \n",
       "1  2015-12-24T00:00:00  small selection italian wines glass beers well...  \n",
       "2  2018-12-28T00:00:00                plus point price remains 4 per bowl  \n",
       "3  2018-03-03T00:00:00            ordered thought signature omelette name  \n",
       "4  2019-01-05T00:00:00  probably get tempted pick soup base sell sure ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the Data\n",
    "def clean(data):\n",
    "\n",
    "    # Removing leading and trailing white spaces\n",
    "    data = str(data).strip()\n",
    "    \n",
    "    # Converting all text to lower case\n",
    "    data = data.lower() \n",
    "    \n",
    "    # add space for punctuation\n",
    "    translator = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    translator.sub(' ', data)\n",
    "\n",
    "    #remove punct\n",
    "    data = re.sub('[^A-Za-z0-9]+', ' ', data)\n",
    "    \n",
    "    #keep english words\n",
    "    data = re.sub(r'[^\\x00-\\x7F]+', '', data)\n",
    "    \n",
    "    #keep printable\n",
    "    data = re.sub(f'[^{re.escape(string.printable)}]', '', data)\n",
    "\n",
    "    # Removing Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(data)\n",
    "\n",
    "    # Converting all text to base form\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens_lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            \n",
    "    # Removing Punctuation\n",
    "#     translator = str.maketrans('', '', string.punctuation)\n",
    "#     data = data.translate(translator)\n",
    "\n",
    "         # Converting all numbers to words\n",
    "#    word = data.split(' ')\n",
    "#     p = inflect.engine()\n",
    "#     word = ' '.join([i for i in word if not i.isdigit()])\n",
    "#     word = re.sub(r'\\d+', p.number_to_words(word) , word)    \n",
    "\n",
    "    word = ' '.join([i for i in tokens if not i in stop_words])\n",
    "\n",
    "    return word\n",
    "\n",
    "# clean text data\n",
    "reviews[\"Review_clean\"] = reviews[\"Review\"].apply(lambda x: clean(x))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       fresh ingredients friendly peeps much cheaper ...\n",
       "1       small selection italian wines glass beers well...\n",
       "2                     plus point price remains 4 per bowl\n",
       "3                 ordered thought signature omelette name\n",
       "4       probably get tempted pick soup base sell sure ...\n",
       "                              ...                        \n",
       "2825                                   went weekday night\n",
       "2826                      came birthday celebration mates\n",
       "2827                                   caters vegetarians\n",
       "2828    friend really wanted come back try foods unfor...\n",
       "2829                          issue place drive take taxi\n",
       "Name: Review_clean, Length: 2830, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.Review_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean(data):\n",
    "    \n",
    "#     # Removing leading and trailing white spaces\n",
    "# #     data = str(data).strip()\n",
    "\n",
    "# # add space for punctuation\n",
    "#     translator = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "#     translator.sub(' ', data)\n",
    "#     # Converting all text to lower case\n",
    "#     data = data.lower() \n",
    "    \n",
    "#     #remove punct\n",
    "#     data = re.sub('[^A-Za-z0-9]+', ' ', data)\n",
    "    \n",
    "#     #keep english words\n",
    "#     data = re.sub(r'[^\\x00-\\x7F]+', '', data)\n",
    "#     #keep printable\n",
    "#     data = re.sub(f'[^{re.escape(string.printable)}]', '', data)\n",
    "    \n",
    "#     # Converting all numbers to words\n",
    "# #     word = data.split(' ')\n",
    "# #     p = inflect.engine()\n",
    "# #     word = ' '.join([i for i in word if not i.isdigit()])\n",
    "# #     word = re.sub(r'\\d+', p.number_to_words(word) , word) \n",
    "\n",
    "# #     translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "# #     # Removing Punctuation\n",
    "# #     #word = data.split(' ')\n",
    "# #     translator = str.maketrans('', '', string.punctuation)\n",
    "# #     data = data.translate(translator)\n",
    "\n",
    "#     # Removing Stopwords\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = word_tokenize(data)\n",
    "#     data = ' '.join([i for i in tokens if not i in stop_words])\n",
    "    \n",
    "#     # Converting all text to base form\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens_lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#     return data\n",
    "\n",
    "# # clean text data\n",
    "# df['review_clean'] = df['Review'].apply(lambda x: clean(x))\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = reviews.Review_clean\n",
    "y = reviews.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split x and y into training and testing sets\n",
    "# stratify returns training and test subsets that have the same proportions of class labels as the input dataset.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    687\n",
      "5    484\n",
      "3    394\n",
      "1    295\n",
      "4    262\n",
      "Name: Label, dtype: int64\n",
      "2    229\n",
      "5    161\n",
      "3    131\n",
      "1     99\n",
      "4     88\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# examine the object shapes\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(x_train)\n",
    "x_train_dtm = vect.transform(x_train)\n",
    "\n",
    "# only transform x_test\n",
    "x_test_dtm = vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1444, 3695)\n",
      "(482, 3695)\n"
     ]
    }
   ],
   "source": [
    "# examine the shapes: rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
    "print(x_train_dtm.shape)\n",
    "print(x_test_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['workers', 'working', 'workout', 'workplace', 'works', 'world', 'worse', 'worst', 'worth', 'worthy', 'would', 'wow', 'wowed', 'wrap', 'wrapped', 'written', 'wrong', 'wrongly', 'xeo', 'xie', 'xlb', 'xo', 'yahoo', 'yakitori', 'yam', 'yamazaki', 'yang', 'year', 'years', 'yelp', 'yelper', 'yeoh', 'yes', 'yet', 'yo', 'yoghurt', 'yogurt', 'yolk', 'yong', 'yu', 'yuan', 'yuen', 'yummier', 'yummmmmmyyyy', 'yummmy', 'yummy', 'yuzu', 'zach', 'zealand', 'zucchini']\n"
     ]
    }
   ],
   "source": [
    "# examine the last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the accuracy of different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "vects = [CountVectorizer(),TfidfVectorizer()]\n",
    "vectnames = [\"Count Vect\",\"Tfidf Vect\"]\n",
    "\n",
    "\n",
    "clfs = [BaggingClassifier(RandomForestClassifier()), MultinomialNB(),LinearSVC(),LogisticRegression(),RandomForestClassifier(),linear_model.SGDClassifier(), GradientBoostingClassifier()]\n",
    "clfnames = [\"bagging random forest\", \"Naive Bayes\",\"Linear SVM\",\"Logistic Regression\",\"Random Forest\",\"Stochastic Gradient Descent\", \"Gradient Boosting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vect + bagging random forest - train acc: 0.9300554016620498 test acc: 0.6203319502074689 \n",
      "Count Vect + Naive Bayes - train acc: 0.9335180055401662 test acc: 0.6991701244813278 \n",
      "Count Vect + Linear SVM - train acc: 0.997229916897507 test acc: 0.6721991701244814 \n",
      "Count Vect + Logistic Regression - train acc: 0.9840720221606648 test acc: 0.6970954356846473 \n",
      "Count Vect + Random Forest - train acc: 0.9847645429362881 test acc: 0.6307053941908713 \n",
      "Count Vect + Stochastic Gradient Descent - train acc: 0.9986149584487535 test acc: 0.6784232365145229 \n",
      "Count Vect + Gradient Boosting - train acc: 0.8795013850415513 test acc: 0.6721991701244814 \n",
      "Tfidf Vect + bagging random forest - train acc: 0.9425207756232687 test acc: 0.6535269709543569 \n",
      "Tfidf Vect + Naive Bayes - train acc: 0.871191135734072 test acc: 0.5871369294605809 \n",
      "Tfidf Vect + Linear SVM - train acc: 0.9958448753462604 test acc: 0.7095435684647303 \n",
      "Tfidf Vect + Logistic Regression - train acc: 0.9023545706371191 test acc: 0.6721991701244814 \n",
      "Tfidf Vect + Random Forest - train acc: 0.981994459833795 test acc: 0.6369294605809128 \n",
      "Tfidf Vect + Stochastic Gradient Descent - train acc: 0.9965373961218836 test acc: 0.6867219917012448 \n",
      "Tfidf Vect + Gradient Boosting - train acc: 0.9037396121883656 test acc: 0.6763485477178424 \n"
     ]
    }
   ],
   "source": [
    "#building a pipeline\n",
    "\n",
    "for vectname, vect, in zip(vectnames, vects):\n",
    "\n",
    "    for clfname, clf in zip(clfnames, clfs):\n",
    "        pipe = Pipeline([\n",
    "            ('vect', vect),\n",
    "            ('clf', clf),\n",
    "        ]\n",
    "        )       \n",
    "\n",
    "        pipe.fit(x_train, y_train)\n",
    "        pred = pipe.predict(x_test)\n",
    "        train_acc = metrics.accuracy_score(y_train, pipe.predict(x_train))\n",
    "        test_acc = metrics.accuracy_score(y_test, pred)\n",
    "        #roc_auc = metrics.roc_auc_score(y_test, pred)\n",
    "        print(\"{} + {} - train acc: {} test acc: {} \".format(vectname, clfname, train_acc, test_acc))\n",
    "        #scores = cross_val_score(pipe, x, y,scoring='roc_auc')\n",
    "        #print(scores.mean())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.689751 using {'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 3), 'tfidf__max_df': 0.75, 'tfidf__binary': True, 'tfidf__analyzer': 'word', 'lr__max_iter': 200, 'lr__dual': True, 'lr__C': 59.94842503189409}\n",
      "Execution time: 6.650973796844482 ms\n",
      "test acc: 0.7012448132780082 recall: 0.6678931772663853 f1: 0.68085743258033\n"
     ]
    }
   ],
   "source": [
    "# doing randomised search \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__analyzer':['char', 'word', 'char_wb'],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidf__binary': [True, False],\n",
    "    'tfidf__norm': [None, 'l1', 'l2'], \n",
    "    'lr__dual': [True,False],\n",
    "    'lr__max_iter': [100,200, 500],\n",
    "    'lr__C' :np.logspace(0, 4, num=10)\n",
    "  \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "random =RandomizedSearchCV(pipe, parameters, cv=5, n_jobs=2)\n",
    "\n",
    "start_time = time.time()\n",
    "random_result = random.fit(x_train, y_train)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))\n",
    "print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')\n",
    "\n",
    "pred = random.predict(x_test)\n",
    "test_acc = metrics.accuracy_score(y_test, pred)\n",
    "#roc_auc = metrics.roc_auc_score(y_test, pred)\n",
    "pre_macro = metrics.precision_score(y_test, pred, average=\"macro\")\n",
    "recall_macro = metrics.recall_score(y_test, pred, average=\"macro\")\n",
    "f1_macro = metrics.f1_score(y_test, pred, average=\"macro\")\n",
    "\n",
    "\n",
    "print(\"test acc: {} recall: {} f1: {}\".format(test_acc, recall_macro, f1_macro))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n",
      "Test Accuracy:  0.7053941908713693\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1, 2), max_df=0.75, norm= 'l2', binary = True, analyzer = 'word')\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "\n",
    "\n",
    "lr = LogisticRegression(max_iter = 100, dual = False, C = 10000)\n",
    "lr.fit(x_train_dtm, y_train)\n",
    "y_pred_class = lr.predict(x_test_dtm)\n",
    "\n",
    "# Get the training accuracy\n",
    "print('Training Accuracy: ', metrics.accuracy_score(y_train, lr.predict(x_train_dtm)))\n",
    "# print the accuracy of its predictions\n",
    "print('Test Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9785318559556787\n",
      "Test Accuracy:  0.6556016597510373\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(1, 2), max_df=0.75, norm= 'l2', binary = True, analyzer = 'word')\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "\n",
    "\n",
    "lr = OneVsRestClassifier(SVC())\n",
    "lr.fit(x_train_dtm, y_train)\n",
    "y_pred_class = lr.predict(x_test_dtm)\n",
    "\n",
    "# Get the training accuracy\n",
    "print('Training Accuracy: ', metrics.accuracy_score(y_train, lr.predict(x_train_dtm)))\n",
    "# print the accuracy of its predictions\n",
    "print('Test Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input sequence for running the model\n",
    "# max_len = 150\n",
    "num_words = 1000\n",
    "tok = Tokenizer(num_words= num_words)\n",
    "tok.fit_on_texts(x_train)\n",
    "word_idx = tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming to matrix\n",
    "trainmatrix = tok.texts_to_matrix(x_train)\n",
    "testmatrix = tok.texts_to_matrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "trainlabels = encoder.transform(y_train)\n",
    "testlabels = encoder.transform(y_test)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(trainlabels, num_classes)\n",
    "y_test = to_categorical(testlabels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "model.add(Dense(512, input_shape=(num_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 515,590\n",
      "Trainable params: 515,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the model\n",
    "model.compile(loss='binary_crossentropy',optimizer= 'adam',metrics=['accuracy'])    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1697 samples, validate on 425 samples\n",
      "Epoch 1/5\n",
      "1697/1697 [==============================] - 2s 1ms/step - loss: 0.4031 - accuracy: 0.8361 - val_loss: 0.3585 - val_accuracy: 0.8510\n",
      "Epoch 2/5\n",
      "1697/1697 [==============================] - 1s 540us/step - loss: 0.2973 - accuracy: 0.8725 - val_loss: 0.2832 - val_accuracy: 0.8820\n",
      "Epoch 3/5\n",
      "1697/1697 [==============================] - 1s 457us/step - loss: 0.2019 - accuracy: 0.9187 - val_loss: 0.2384 - val_accuracy: 0.8969\n",
      "Epoch 4/5\n",
      "1697/1697 [==============================] - 1s 312us/step - loss: 0.1388 - accuracy: 0.9518 - val_loss: 0.2209 - val_accuracy: 0.9125\n",
      "Epoch 5/5\n",
      "1697/1697 [==============================] - 1s 480us/step - loss: 0.1045 - accuracy: 0.9672 - val_loss: 0.2246 - val_accuracy: 0.9110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e01b303348>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(trainmatrix,y_train,validation_split = 0.2, \n",
    "           batch_size=32, epochs=5, \n",
    "          callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708/708 [==============================] - 0s 131us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate results for testing data\n",
    "accr = model.evaluate(testmatrix,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 0.237\n",
      "  Accuracy: 0.906\n"
     ]
    }
   ],
   "source": [
    "# Print evaluation results\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
